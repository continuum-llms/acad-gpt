[{
  "renderDpi": 300,
  "name": "2",
  "page": 5,
  "figType": "Table",
  "regionBoundary": {
    "x1": 312.96,
    "y1": 77.75999999999999,
    "x2": 534.0,
    "y2": 268.32
  },
  "caption": "Table 2. Detailed comparison of effective robustness across various datasets. Although both models perform within 0.1% of each other on LibriSpeech, a zero-shot Whisper model performs much better on other datasets than expected for its LibriSpeech performance and makes 55.2% less errors on average. Results reported in word error rate (WER) for both models after applying our text normalizer.",
  "imageText": ["Average", "29.3", "12.8", "55.2", "Artie", "24.5", "6.2", "74.7", "Common", "Voice", "29.9", "9.0", "69.9", "Fleurs", "En", "14.6", "4.4", "69.9", "Tedlium", "10.5", "4.0", "61.9", "CHiME6", "65.8", "25.5", "61.2", "VoxPopuli", "En", "17.9", "7.3", "59.2", "CORAAL", "35.6", "16.2", "54.5", "AMI", "IHM", "37.0", "16.9", "54.3", "Switchboard", "28.3", "13.8", "51.2", "CallHome", "34.8", "17.6", "49.4", "WSJ", "7.7", "3.9", "49.4", "AMI", "SDM1", "67.6", "36.4", "46.2", "LibriSpeech", "Other", "6.2", "5.2", "16.1", "LibriSpeech", "Clean", "2.7", "2.7", "0.0", "wav2vec", "2.0", "Whisper", "RER", "Dataset", "Large", "(no", "LM)", "Large", "V2", "(%)"],
  "renderURL": "/Users/shahrukh/Desktop/acad-gpt/Users/shahrukh/Desktop/acad-gpt/examples/paper_highlights/pdf/figures/de6b744efdf14a3c85ba5cb3f5fc3a77_whisper-Table2-1.png",
  "captionBoundary": {
    "x1": 306.89300537109375,
    "y1": 280.0637512207031,
    "x2": 542.9253540039062,
    "y2": 351.218994140625
  }
}, {
  "renderDpi": 300,
  "name": "2",
  "page": 5,
  "figType": "Figure",
  "regionBoundary": {
    "x1": 59.76,
    "y1": 70.8,
    "x2": 285.12,
    "y2": 289.92
  },
  "caption": "Figure 2. Zero-shot Whisper models close the gap to human robustness. Despite matching or outperforming a human on LibriSpeech dev-clean, supervised LibriSpeech models make roughly twice as many errors as a human on other datasets demonstrating their brittleness and lack of robustness. The estimated robustness frontier of zero-shot Whisper models, however, includes the 95% confidence interval for this particular human.",
  "imageText": ["Supervised", "LibriSpeech", "models", "Zero-shot", "Whisper", "models", "Zero-shot", "Human", "(Alec)", "Ideal", "robustness", "(y", "=", "x)", "40", "50", "Av", "er", "ag", "e", "W", "ER", "o", "n", "[C", "om", "m", "on", "V", "oi", "ce", ",", "C", "Hi", "M", "E-", "6,", "T", "ED", "-L", "IU", "M", "]", "(", "%", ")", "30", "20", "10", "0", "0", "1", "2", "3", "4", "5", "6", "7", "8", "WER", "on", "LibriSpeech", "dev-clean", "(%)"],
  "renderURL": "/Users/shahrukh/Desktop/acad-gpt/Users/shahrukh/Desktop/acad-gpt/examples/paper_highlights/pdf/figures/de6b744efdf14a3c85ba5cb3f5fc3a77_whisper-Figure2-1.png",
  "captionBoundary": {
    "x1": 54.9379997253418,
    "y1": 309.8717346191406,
    "x2": 290.9266357421875,
    "y2": 381.02801513671875
  }
}, {
  "renderDpi": 300,
  "name": "6",
  "page": 10,
  "figType": "Table",
  "regionBoundary": {
    "x1": 68.88,
    "y1": 293.76,
    "x2": 274.08,
    "y2": 401.03999999999996
  },
  "caption": "Table 6. Performance improves with increasing dataset size. English speech recognition performance refers to an average over 12 datasets while the Multilingual speech recognition reports performance on the overlapping subset of languages in Fleurs and X→en translation reports average BLEU on CoVoST2. Dataset size reported in hours.",
  "imageText": ["Dataset", "English", "Multilingual", "X→En", "size", "WER", "(↓)", "WER", "(↓)", "BLEU", "(↑)", "3405", "30.5", "92.4", "0.2", "6811", "19.6", "72.7", "1.7", "13621", "14.4", "56.6", "7.9", "27243", "12.3", "45.0", "13.9", "54486", "10.9", "36.4", "19.2", "681070", "9.9", "29.2", "24.8"],
  "renderURL": "/Users/shahrukh/Desktop/acad-gpt/Users/shahrukh/Desktop/acad-gpt/examples/paper_highlights/pdf/figures/de6b744efdf14a3c85ba5cb3f5fc3a77_whisper-Table6-1.png",
  "captionBoundary": {
    "x1": 54.768001556396484,
    "y1": 412.4467468261719,
    "x2": 291.01177978515625,
    "y2": 472.64300537109375
  }
}, {
  "renderDpi": 300,
  "name": "8",
  "page": 10,
  "figType": "Figure",
  "regionBoundary": {
    "x1": 59.28,
    "y1": 69.36,
    "x2": 538.56,
    "y2": 199.44
  },
  "caption": "Figure 8. Zero-shot Whisper performance scales reliably across tasks and languages with increasing model size. Lightly shaded lines represent individual datasets or languages, showing that performance is more varied than the smooth trends in aggregate performance. Large V2 distinguished with a dashed orange line since it includes several changes that are not present for the smaller models in this analysis.",
  "imageText": ["Language", "Identification", "(Fleurs)", "Average", "Large", "V2", ")", "s", "(", "%", "ua", "ge", "la", "ng", "10", "2", "o", "n", "ra", "cy", "Ac", "cu", "80", "70", "60", "50", "40", "30", "38M", "73M", "244M", "768M", "1549M", "Model", "parameters", "X->En", "Translation", "(CoVoST2)", "Average", "Large", "V2", "ge", "s", "ng", "ua", "21", "la", "o", "n", "BL", "EU", "50", "40", "30", "20", "10", "0", "38M", "73M", "244M", "768M", "1549M", "Model", "parameters", "Multilingual", "Speech", "Recognition", "(Fleurs)", "Average", "Large", "V2", "%", ")", "ge", "s", "(", "ng", "ua", "67", "la", "o", "n", "W", "ER", "100", "80", "60", "40", "20", "0", "38M", "73M", "244M", "768M", "1549M", "Model", "parameters", "English", "Speech", "Recognition", "Average", "Large", "V2", "%", ")", "et", "s", "(", "at", "as", "12", "d", "o", "n", "W", "ER", "20.0", "17.5", "15.0", "12.5", "10.0", "7.5", "5.0", "2.5", "0.0", "38M", "73M", "244M", "768M", "1549M", "Model", "parameters"],
  "renderURL": "/Users/shahrukh/Desktop/acad-gpt/Users/shahrukh/Desktop/acad-gpt/examples/paper_highlights/pdf/figures/de6b744efdf14a3c85ba5cb3f5fc3a77_whisper-Figure8-1.png",
  "captionBoundary": {
    "x1": 54.9379997253418,
    "y1": 217.99276733398438,
    "x2": 543.00830078125,
    "y2": 256.27197265625
  }
}, {
  "renderDpi": 300,
  "name": "4",
  "page": 6,
  "figType": "Figure",
  "regionBoundary": {
    "x1": 311.76,
    "y1": 70.8,
    "x2": 537.36,
    "y2": 281.28
  },
  "caption": "Figure 4. Correlation of pre-training supervision amount with downstream translation performance. The amount of pretraining translation data for a given language is only moderately predictive of Whisper’s zero-shot performance on that language in Fleurs.",
  "imageText": ["TA", "GU", "HI", "SL", "CS", "SO", "SK", "PS", "TR", "MN", "SN", "NBBS", "JA", "ZH", "ES", "SR", "MK", "MS", "RU", "KA", "DE", "LT", "PL", "HY", "CY", "LB", "YO", "AZ", "LV", "LN", "UR", "ID", "SD", "KO", "HU", "FI", "PT", "BN", "LO", "MT", "HA", "VI", "FRAF", "HE", "KK", "IS", "CA", "OC", "ET", "AS", "TG", "KM", "BE", "UZ", "FA", "UK", "GL", "RO", "FIL", "IT", "SV", "MR", "TE", "ML", "BG", "MI", "AR", "DA", "PA", "KN", "NE", "TH", "EL", "SW", "MY", "NL", "AM", "HR", "r2", "=", "0.24", "BL", "EU", "40", "35", "30", "25", "20", "15", "10", "5", "0", "1", "10", "100", "1K", "10K", "100K", "Hours", "of", "translated", "audio"],
  "renderURL": "/Users/shahrukh/Desktop/acad-gpt/Users/shahrukh/Desktop/acad-gpt/examples/paper_highlights/pdf/figures/de6b744efdf14a3c85ba5cb3f5fc3a77_whisper-Figure4-1.png",
  "captionBoundary": {
    "x1": 306.93798828125,
    "y1": 302.0737609863281,
    "x2": 542.927490234375,
    "y2": 351.3110046386719
  }
}, {
  "renderDpi": 300,
  "name": "3",
  "page": 6,
  "figType": "Figure",
  "regionBoundary": {
    "x1": 59.519999999999996,
    "y1": 70.56,
    "x2": 285.36,
    "y2": 282.96
  },
  "caption": "Figure 3. Correlation of pre-training supervision amount with downstream speech recognition performance. The amount of pre-training speech recognition data for a given language is very predictive of zero-shot performance on that language in Fleurs.",
  "imageText": ["SR", "ET", "TG", "KK", "ES", "DA", "PA", "MR", "IT", "HE", "BE", "MT", "KN", "HI", "BGFIL", "EL", "RU", "CY", "SK", "SL", "ZH", "KA", "BS", "UR", "HU", "EN", "BN", "KM", "TH", "HY", "FA", "TA", "SV", "PL", "ID", "LV", "VI", "DE", "UZ", "HR", "AF", "AR", "NB", "CS", "TR", "TE", "CA", "MY", "IS", "GU", "MS", "NL", "LT", "MK", "AZ", "LO", "NE", "UK", "KO", "RO", "GL", "FR", "ML", "FI", "JA", "PT", "SW", "r2", "=", "0.83", "80", "160", "W", "or", "d", "Er", "ro", "r", "R", "at", "e", "(W", "ER", ")", "40", "20", "10", "5", "2.5", "0.1", "1", "10", "100", "1K", "10K", "100K", "1M", "Hours", "of", "transcribed", "audio"],
  "renderURL": "/Users/shahrukh/Desktop/acad-gpt/Users/shahrukh/Desktop/acad-gpt/examples/paper_highlights/pdf/figures/de6b744efdf14a3c85ba5cb3f5fc3a77_whisper-Figure3-1.png",
  "captionBoundary": {
    "x1": 54.9379997253418,
    "y1": 303.7487487792969,
    "x2": 289.7575378417969,
    "y2": 342.02801513671875
  }
}, {
  "renderDpi": 300,
  "name": "3",
  "page": 6,
  "figType": "Table",
  "regionBoundary": {
    "x1": 81.84,
    "y1": 373.91999999999996,
    "x2": 260.15999999999997,
    "y2": 462.24
  },
  "caption": "Table 3. Multilingual speech recognition performance. Zeroshot Whisper improves performance on Multilingual LibriSpeech (MLS) but is still significantly behind both Maestro, XLS-R, and mSLAM on VoxPopuli.",
  "imageText": ["Zero-Shot", "Whisper", "7.3", "13.6", "VP-10K", "+", "FT", "-", "15.3", "XLS-R", "(1B)", "10.9", "10.6", "mSLAM-CTC", "(2B)", "9.7", "9.1", "Maestro", "-", "8.1", "Model", "MLS", "VoxPopuli"],
  "renderURL": "/Users/shahrukh/Desktop/acad-gpt/Users/shahrukh/Desktop/acad-gpt/examples/paper_highlights/pdf/figures/de6b744efdf14a3c85ba5cb3f5fc3a77_whisper-Table3-1.png",
  "captionBoundary": {
    "x1": 54.893001556396484,
    "y1": 473.9537353515625,
    "x2": 290.9286804199219,
    "y2": 512.2330322265625
  }
}, {
  "renderDpi": 300,
  "name": "9",
  "page": 21,
  "figType": "Table",
  "regionBoundary": {
    "x1": 98.88,
    "y1": 537.84,
    "x2": 496.08,
    "y2": 696.0
  },
  "caption": "Table 9. English transcription WER (%) with beam search and temperature fallback",
  "imageText": ["Whisper", "tiny.en", "5.4", "12.8", "5.4", "4.6", "21.4", "16.0", "23.5", "18.4", "21.4", "42.0", "22.7", "54.2", "10.9", "10.0", "Whisper", "tiny", "6.7", "15.0", "6.3", "5.9", "24.8", "18.3", "26.1", "20.8", "25.1", "48.0", "25.6", "57.3", "11.6", "12.4", "Whisper", "base.en", "4.1", "9.6", "4.6", "4.0", "18.3", "14.2", "17.5", "13.2", "18.5", "35.2", "21.1", "49.0", "9.3", "7.1", "Whisper", "base", "4.9", "11.0", "5.0", "4.4", "20.5", "15.6", "19.4", "15.3", "20.5", "40.0", "21.5", "50.0", "9.5", "8.9", "Whisper", "small.en", "3.2", "6.7", "4.3", "3.0", "17.2", "13.4", "12.6", "9.2", "17.5", "29.5", "17.9", "42.5", "8.1", "5.3", "Whisper", "small", "3.3", "7.2", "4.3", "3.9", "17.1", "13.3", "12.8", "9.3", "16.4", "30.9", "19.2", "43.5", "8.2", "6.1", "Whisper", "medium.en", "3.0", "5.7", "4.3", "2.8", "14.7", "12.4", "10.3", "7.4", "15.3", "27.0", "17.1", "39.4", "7.8", "4.5", "Whisper", "medium", "2.7", "5.6", "4.0", "2.7", "15.3", "13.2", "9.7", "6.7", "14.9", "27.6", "17.6", "43.0", "7.6", "4.4", "Whisper", "large", "2.8", "5.7", "4.3", "3.5", "16.2", "14.2", "8.9", "6.4", "15.1", "25.2", "17.6", "37.1", "7.2", "4.5", "Whisper", "large-v2", "2.5", "4.9", "3.7", "2.6", "16.4", "13.6", "8.2", "5.7", "14.2", "24.9", "17.4", "39.9", "7.0", "4.2", "Fleurs.en", "us", "VoxPopuli.en", "A", "M", "I-SD", "M", "1", "A", "M", "I-IH", "M", "C", "H", "iM", "E", "6", "C", "O", "R", "A", "A", "L", "A", "rtie", "C", "om", "m", "onVoice5.1", "Sw", "itchboard", "C", "allH", "om", "e", "W", "SJ", "T", "E", "D", "-L", "IU", "M", "3", "L", "ibriSpeech.test-other", "L", "ibriSpeech.test-clean", "Model"],
  "renderURL": "/Users/shahrukh/Desktop/acad-gpt/Users/shahrukh/Desktop/acad-gpt/examples/paper_highlights/pdf/figures/de6b744efdf14a3c85ba5cb3f5fc3a77_whisper-Table9-1.png",
  "captionBoundary": {
    "x1": 148.0199432373047,
    "y1": 707.7178344726562,
    "x2": 448.864501953125,
    "y2": 713.1201171875
  }
}, {
  "renderDpi": 300,
  "name": "7",
  "page": 9,
  "figType": "Figure",
  "regionBoundary": {
    "x1": 310.8,
    "y1": 69.84,
    "x2": 538.0799999999999,
    "y2": 205.92
  },
  "caption": "Figure 7. Whisper’s performance is close to that of professional human transcribers. This plot shows the WER distributions of 25 recordings from the Kincaid46 dataset transcribed by Whisper, the same 4 commercial ASR systems from Figure 6 (A-D), one computer-assisted human transcription service (E) and 4 human transcription services (F-I). The box plot is superimposed with dots indicating the WERs on individual recordings, and the aggregate WER over the 25 recordings are annotated on each box.",
  "imageText": ["(%", ")", "at", "e", "ro", "r", "R", "d", "Er", "W", "or", "30", "25", "20", "15", "10", "5", "0", "computer-assisted", "Whisper", "A", "B", "C", "D", "E", "F", "G", "H", "I", "ASR", "human", "transcription"],
  "renderURL": "/Users/shahrukh/Desktop/acad-gpt/Users/shahrukh/Desktop/acad-gpt/examples/paper_highlights/pdf/figures/de6b744efdf14a3c85ba5cb3f5fc3a77_whisper-Figure7-1.png",
  "captionBoundary": {
    "x1": 306.93798828125,
    "y1": 224.93075561523438,
    "x2": 542.5631103515625,
    "y2": 307.0450134277344
  }
}, {
  "renderDpi": 300,
  "name": "17",
  "page": 27,
  "figType": "Table",
  "regionBoundary": {
    "x1": 182.88,
    "y1": 93.6,
    "x2": 412.08,
    "y2": 272.4
  },
  "caption": "Table 17. Whisper training hyperparameters.",
  "imageText": ["Updates", "1048576", "Batch", "Size", "256", "Warmup", "Updates", "2048", "Max", "grad", "norm", "1.0", "Optimizer", "AdamW", "β1", "0.9", "β2", "0.98", "ϵ", "10−6", "Weight", "Decay", "0.1", "Weight", "Init", "Gaussian", "Fan-In", "Learning", "Rate", "Schedule", "Linear", "Decay", "Speechless", "audio", "subsample", "factor", "10×", "Condition", "on", "prior", "text", "rate", "50%", "Hyperparameter", "Value"],
  "renderURL": "/Users/shahrukh/Desktop/acad-gpt/Users/shahrukh/Desktop/acad-gpt/examples/paper_highlights/pdf/figures/de6b744efdf14a3c85ba5cb3f5fc3a77_whisper-Table17-1.png",
  "captionBoundary": {
    "x1": 217.97000122070312,
    "y1": 283.99176025390625,
    "x2": 378.9115295410156,
    "y2": 289.3940124511719
  }
}, {
  "renderDpi": 300,
  "name": "19",
  "page": 27,
  "figType": "Table",
  "regionBoundary": {
    "x1": 226.79999999999998,
    "y1": 442.8,
    "x2": 367.2,
    "y2": 538.0799999999999
  },
  "caption": "Table 19. Whisper model learning rates.",
  "imageText": ["Tiny", "1.5×", "10−3", "Base", "1×", "10−3", "Small", "5×", "10−4", "Medium", "2.5×", "10−4", "Large", "1.75×", "10−4", "Large", "V2", "2.0×", "10−4", "Model", "Max", "Learning", "Rate"],
  "renderURL": "/Users/shahrukh/Desktop/acad-gpt/Users/shahrukh/Desktop/acad-gpt/examples/paper_highlights/pdf/figures/de6b744efdf14a3c85ba5cb3f5fc3a77_whisper-Table19-1.png",
  "captionBoundary": {
    "x1": 226.28700256347656,
    "y1": 549.4517211914062,
    "x2": 370.595947265625,
    "y2": 554.85400390625
  }
}, {
  "renderDpi": 300,
  "name": "18",
  "page": 27,
  "figType": "Table",
  "regionBoundary": {
    "x1": 205.92,
    "y1": 315.84,
    "x2": 388.08,
    "y2": 399.12
  },
  "caption": "Table 18. Hyperparameters changed for Whisper Large V2.",
  "imageText": ["Updates", "655360", "Batch", "Size", "1024", "BPE", "Dropout", "0.1", "Stochastic", "Depth", "0.1", "SpecAugment", "Policy", "LibriSpeech", "Basic", "Hyperparameter", "Value"],
  "renderURL": "/Users/shahrukh/Desktop/acad-gpt/Users/shahrukh/Desktop/acad-gpt/examples/paper_highlights/pdf/figures/de6b744efdf14a3c85ba5cb3f5fc3a77_whisper-Table18-1.png",
  "captionBoundary": {
    "x1": 191.52000427246094,
    "y1": 410.7447509765625,
    "x2": 405.3634948730469,
    "y2": 416.1470031738281
  }
}, {
  "renderDpi": 300,
  "name": "7",
  "page": 12,
  "figType": "Table",
  "regionBoundary": {
    "x1": 55.919999999999995,
    "y1": 77.75999999999999,
    "x2": 289.2,
    "y2": 191.04
  },
  "caption": "Table 7. Long-form transcription performance improves incrementally as additional decoding heuristics are employed. Details on each intervention are described in Section 4.5.",
  "imageText": ["Greedy", "decoding", "only", "3.95", "5.16", "9.69", "11.7", "10.7", "14.0", "22.0", "11.0", "+", "Beam", "search", "4.16", "5.71", "9.42", "11.5", "10.2", "13.4", "20.0", "10.6", "+", "Temperature", "fallback", "4.16", "5.71", "9.42", "11.5", "10.2", "13.4", "20.0", "10.6", "+", "Voice", "activity", "detection", "3.56", "4.61", "9.45", "11.4", "10.1", "13.2", "19.4", "10.2", "+", "Previous", "text", "conditioning", "3.42", "6.16", "8.72", "11.0", "9.63", "13.3", "18.1", "10.0", "+", "Initial", "timestamp", "constraint", "3.51", "5.26", "8.41", "11.5", "9.73", "12.6", "19.1", "10.0", "ra", "ge", "A", "ve", "A", "L", "R", "A", "C", "O", "s-", "22", "ni", "ng", "E", "ar", "s-", "21", "ni", "ng", "E", "ar", "16", "R", "ev", "46", "ca", "id", "K", "in", "le", "nw", "hi", "M", "ea", "3", "IU", "M", "D", "-L", "T", "E"],
  "renderURL": "/Users/shahrukh/Desktop/acad-gpt/Users/shahrukh/Desktop/acad-gpt/examples/paper_highlights/pdf/figures/de6b744efdf14a3c85ba5cb3f5fc3a77_whisper-Table7-1.png",
  "captionBoundary": {
    "x1": 54.89297103881836,
    "y1": 202.14273071289062,
    "x2": 290.9333190917969,
    "y2": 229.4630126953125
  }
}, {
  "renderDpi": 300,
  "name": "4",
  "page": 7,
  "figType": "Table",
  "regionBoundary": {
    "x1": 64.8,
    "y1": 77.75999999999999,
    "x2": 277.2,
    "y2": 166.07999999999998
  },
  "caption": "Table 4. X→en Speech translation performance. Zero-shot Whisper outperforms existing models on CoVoST2 in the overall, medium, and low resource settings but still moderately underperforms on high-resource languages compared to prior directly supervised work.",
  "imageText": ["Zero-Shot", "Whisper", "36.2", "32.6", "25.2", "29.1", "XMEF-X", "34.2", "20.2", "5.9", "14.7", "XLS-R", "(2B)", "36.1", "27.7", "15.1", "22.1", "mSLAM-CTC", "(2B)", "37.8", "29.6", "18.5", "24.8", "Maestro", "38.2", "31.3", "18.4", "25.2", "X→", "English", "High", "Mid", "Low", "All"],
  "renderURL": "/Users/shahrukh/Desktop/acad-gpt/Users/shahrukh/Desktop/acad-gpt/examples/paper_highlights/pdf/figures/de6b744efdf14a3c85ba5cb3f5fc3a77_whisper-Table4-1.png",
  "captionBoundary": {
    "x1": 54.893001556396484,
    "y1": 177.42776489257812,
    "x2": 290.933349609375,
    "y2": 226.666015625
  }
}, {
  "renderDpi": 300,
  "name": "5",
  "page": 7,
  "figType": "Figure",
  "regionBoundary": {
    "x1": 310.08,
    "y1": 68.88,
    "x2": 539.04,
    "y2": 292.08
  },
  "caption": "Figure 5. WER on LibriSpeech test-clean as a function of SNR under additive white noise (left) and pub noise (right). The accuracy of LibriSpeech-trained models degrade faster than the best Whisper model (⋆). NVIDIA STT models (•) perform best under low noise but are outperformed by Whisper under high noise (SNR < 10 dB). The second-best model under low noise (▼) is fine-tuned on LibriSpeech only and degrades even more quickly.",
  "imageText": ["unispeech-sat-base-100h-libri-ft", "wav2vec2-base-100h", "wav2vec2-base-960h", "wav2vec2-large-960h", "wav2vec2-large-robust-ft-libri-960h", "wav2vec2-large-960h-lv60-self", "asr-crdnn-rnnlm-librispeech", "asr-transformer-transformerlm-librispeech", "hubert-large-ls960-ft", "hubert-xlarge-ls960-ft", "s2t-medium-librispeech-asr", "s2t-large-librispeech-asr", "stt_en_conformer_ctc_large", "stt_en_conformer_transducer_xlarge", "Whisper", "pub", "noise", "40", "30", "20", "10", "0", "-10", "signal-to-noise", "ratio", "(dB)", "white", "noise", "(%", ")", "le", "an", "st", "-c", "h", "te", "pe", "ec", "Lib", "riS", "o", "n", "W", "ER", "100", "50", "20", "10", "5", "2", "1", "40", "30", "20", "10", "0", "-10", "signal-to-noise", "ratio", "(dB)"],
  "renderURL": "/Users/shahrukh/Desktop/acad-gpt/Users/shahrukh/Desktop/acad-gpt/examples/paper_highlights/pdf/figures/de6b744efdf14a3c85ba5cb3f5fc3a77_whisper-Figure5-1.png",
  "captionBoundary": {
    "x1": 306.93798828125,
    "y1": 309.6467590332031,
    "x2": 541.444580078125,
    "y2": 380.8030090332031
  }
}, {
  "renderDpi": 300,
  "name": "5",
  "page": 7,
  "figType": "Table",
  "regionBoundary": {
    "x1": 106.8,
    "y1": 248.88,
    "x2": 236.16,
    "y2": 313.2
  },
  "caption": "Table 5. Language identification performance. Zero-shot Whisper’s accuracy at language identification is not competitive with prior supervised results on Fleurs. This is partially due to Whisper being heavily penalized for having no training data for 20 of Fleurs languages.",
  "imageText": ["Zero-shot", "Whisper", "64.5", "w2v-bert-51", "(0.6B)", "71.4", "mSLAM-CTC", "(2B)", "77.7", "Language", "ID", "Fleurs"],
  "renderURL": "/Users/shahrukh/Desktop/acad-gpt/Users/shahrukh/Desktop/acad-gpt/examples/paper_highlights/pdf/figures/de6b744efdf14a3c85ba5cb3f5fc3a77_whisper-Table5-1.png",
  "captionBoundary": {
    "x1": 54.893001556396484,
    "y1": 324.728759765625,
    "x2": 290.9308776855469,
    "y2": 373.9670104980469
  }
}, {
  "renderDpi": 300,
  "name": "1",
  "page": 3,
  "figType": "Figure",
  "regionBoundary": {
    "x1": 55.919999999999995,
    "y1": 66.96,
    "x2": 541.1999999999999,
    "y2": 433.2
  },
  "caption": "Figure 1. Overview of our approach. A sequence-to-sequence Transformer model is trained on many different speech processing tasks, including multilingual speech recognition, speech translation, spoken language identification, and voice activity detection. All of these tasks are jointly represented as a sequence of tokens to be predicted by the decoder, allowing for a single model to replace many different stages of a traditional speech processing pipeline. The multitask training format uses a set of special tokens that serve as task specifiers or classification targets, as further explained in Section 2.3.",
  "imageText": ["TRANS-", "CRIBE", "self", "attention", "cross", "attention", "MLP", "self", "attention", "cross", "attention", "MLP", "self", "attention", "cross", "attention", "MLP", "self", "attention", "MLP", "self", "attention", "MLP", "self", "attention", "MLP", "X", "→", "X", "Transcription", "Language", "identification", "previous", "text", "tokens", "X", "→", "English", "Translation", "Text-only", "transcription", "(allows", "dataset-specific", "fine-tuning)", "Time-aligned", "transcription", "Custom", "vocabulary", "/", "prompting", "(VAD)", "Voice", "activity", "detection", "text", "tokens", "⋯end", "timetext", "tokens", "begin", "time", "end", "timetext", "tokens", "NO", "TIMESTAMPS", "begin", "time", "TRANSLATE", "TRANSCRIBE", "EOT", "NO", "SPEECH", "LANGUAGE", "TAG", "START", "OF", "TRANSCRIPT", "timestamp", "tokens", "text", "tokens", "special", "tokens", "PREV", "(background", "music", "playing)", "∅", "언덕", "위에", "올라", "내려다보면", "너무나", "넓고", "넓은", "⋯", "“언덕", "위에", "올라", "내려다보면", "너무나", "넓고", "넓은", "⋯”", "The", "quick", "brown", "fox", "jumps", "over", "⋯", "“El", "rápido", "zorro", "marrón", "salta", "sobre", "⋯”", "Ask", "not", "what", "your", "country", "can", "do", "for", "⋯", "“Ask", "not", "what", "your", "country", "can", "do", "for", "⋯”", "No", "speech", "Non-English", "transcription", "Any-to-English", "speech", "translation", "English", "transcription", "Multitask", "training", "format", "Multitask", "training", "data", "(680k", "hours)", "Sequence-to-sequence", "learning", "Learned", "Positional", "Encoding", "Sinusoidal", "Positional", "Encoding", "next-token", "prediction", "⋮", "⋮", "EN", "0.0", "The", "quick", "brown", "Decoder", "Blocks", "Transformer", "Encoder", "Blocks", "Transformer", "Tokens", "in", "Multitask", "Training", "Format", "~", "SOT", "EN", "TRANS-", "CRIBE", "0.0", "The", "quick", "Log-Mel", "Spectrogram", "io", "n", "te", "nt", "s", "at", "cr", "os", "⋮", "2", "×", "Conv1D", "+", "GELU", "⋯", "⋯"],
  "renderURL": "/Users/shahrukh/Desktop/acad-gpt/Users/shahrukh/Desktop/acad-gpt/examples/paper_highlights/pdf/figures/de6b744efdf14a3c85ba5cb3f5fc3a77_whisper-Figure1-1.png",
  "captionBoundary": {
    "x1": 54.9379997253418,
    "y1": 449.7907409667969,
    "x2": 542.5579833984375,
    "y2": 499.02801513671875
  }
}, {
  "renderDpi": 300,
  "name": "10",
  "page": 11,
  "figType": "Figure",
  "regionBoundary": {
    "x1": 310.8,
    "y1": 69.84,
    "x2": 538.0799999999999,
    "y2": 270.96
  },
  "caption": "Figure 10. On most datasets, our text normalizer has similar effect on reducing WERs between Whisper models and other open-source models, compared to FairSpeech’s normalizer. For each dataset, the boxplot shows the distribution of relative WER reduction across different models in our eval suite, showing that using our text normalizer generally results in lower WERs than FairSpeech’s. On a few datasets our normalizer reduces WER significantly and more so for Whisper models, such as CallHome and Switchboard which have many contractions in the ground truth and WSJ which contains many numerical expressions.",
  "imageText": ["Open-source", "models", "Whisper", "models", "CallHome", "Switchboard", "VoxPopuli.en", "WSJ", "Artie", "LibriSpeech", "TED-LIUM3", "Fleurs.en_us", "AMI-IHM", "AMI-SDM1", "CommonVoice5.1", "CORAAL", "CommonVoice9.en", "01020304050", "Relative", "WER", "reduction", "compared", "to", "FairSpeech's", "normalizer", "(%)"],
  "renderURL": "/Users/shahrukh/Desktop/acad-gpt/Users/shahrukh/Desktop/acad-gpt/examples/paper_highlights/pdf/figures/de6b744efdf14a3c85ba5cb3f5fc3a77_whisper-Figure10-1.png",
  "captionBoundary": {
    "x1": 306.93798828125,
    "y1": 290.07574462890625,
    "x2": 541.6404418945312,
    "y2": 394.1080017089844
  }
}, {
  "renderDpi": 300,
  "name": "9",
  "page": 11,
  "figType": "Figure",
  "regionBoundary": {
    "x1": 59.76,
    "y1": 70.8,
    "x2": 285.12,
    "y2": 287.76
  },
  "caption": "Figure 9. Multitask and multilingual transfer improves with scale. For small models, performance on English speech recognition degrades when trained jointly in a multitask and multilingual setup. However, multilingual and multitask models benefit more from scale and eventually outperform models trained on English data only. 95% bootstrap estimate confidence intervals are shown.",
  "imageText": ["English", "Only", "Multilingual", "and", "Multitask", "18", "20", "Av", "er", "ag", "e", "W", "ER", "o", "n", "11", "e", "ng", "lis", "h", "sp", "ee", "ch", "re", "co", "gn", "iti", "on", "d", "at", "as", "et", "s", "16", "14", "12", "10", "8", "10e+19", "10e+20", "10e+21", "10e+22", "FLOPs", "training", "on", "english", "speech", "recognition"],
  "renderURL": "/Users/shahrukh/Desktop/acad-gpt/Users/shahrukh/Desktop/acad-gpt/examples/paper_highlights/pdf/figures/de6b744efdf14a3c85ba5cb3f5fc3a77_whisper-Figure9-1.png",
  "captionBoundary": {
    "x1": 54.9379997253418,
    "y1": 307.8077392578125,
    "x2": 290.9273681640625,
    "y2": 368.0050048828125
  }
}, {
  "renderDpi": 300,
  "name": "11",
  "page": 26,
  "figType": "Figure",
  "regionBoundary": {
    "x1": 366.96,
    "y1": 96.24,
    "x2": 538.0799999999999,
    "y2": 573.84
  },
  "caption": "Figure 11. Training dataset statistics",
  "imageText": ["Turkmen", "1", "Bashkir", "1", "Malagasy", "2", "Uzbek", "4", "Sundanese", "7", "Hausa", "8", "Luxembourgish", "10", "Tatar", "14", "Tajik", "15", "Lingala", "20", "Lao", "20", "Somali", "21", "Macedonian", "30", "Kazakh", "31", "Amharic", "32", "Georgian", "40", "Maltese", "41", "Sindhi", "46", "Faroese", "46", "Occitan", "49", "Burmese", "59", "Pashto", "63", "Latvian", "68", "Albanian", "72", "Haitian", "Creole", "74", "Estonian", "79", "Mongolian", "79", "Icelandic", "84", "Yiddish", "85", "Azerbaijani", "86", "Kannada", "90", "Lithuanian", "99", "Armenian", "116", "Punjabi", "117", "Belarusian", "133", "Nepali", "133", "Assamese", "136", "Serbian", "136", "Slovak", "144", "Basque", "168", "Tibetan", "186", "Sanskrit", "195", "Bulgarian", "202", "Gujarati", "208", "Sinhala", "211", "Bosnian", "219", "Catalan", "236", "Croatian", "239", "Breton", "269", "Shona", "279", "Swahili", "282", "Marathi", "288", "Norwegian", "322", "Afrikaans", "330", "Hawaiian", "338", "Galician", "368", "Danish", "386", "Persian", "392", "Slovenian", "395", "Czech", "401", "Hebrew", "418", "Yoruba", "432", "Ukrainian", "509", "Hungarian", "554", "Romanian", "555", "Javanese", "622", "Khmer", "672", "Finnish", "750", "Malayalam", "892", "Tagalog", "894", "Greek", "968", "Telugu", "987", "Swedish", "1055", "Indonesian", "1174", "Maori", "1381", "Tamil", "1484", "Latin", "1614", "Thai", "1635", "Malay", "1691", "Vietnamese", "1719", "Dutch", "1767", "Norwegian", "Nynorsk", "1889", "Bengali", "1988", "Urdu", "1990", "Italian", "2145", "Polish", "2200", "Turkish", "2241", "Arabic", "2286", "Portuguese", "3620", "German", "4309", "French", "4481", "Hindi", "5438", "Spanish", "6693", "Russian", "7687", "Welsh", "8263", "Japanese", "8860", "Chinese", "11731", "Korean", "19938", "Translation", "1", "10", "100", "1K", "10K", "Hours", "of", "audio"],
  "renderURL": "/Users/shahrukh/Desktop/acad-gpt/Users/shahrukh/Desktop/acad-gpt/examples/paper_highlights/pdf/figures/de6b744efdf14a3c85ba5cb3f5fc3a77_whisper-Figure11-1.png",
  "captionBoundary": {
    "x1": 233.0469970703125,
    "y1": 594.4567260742188,
    "x2": 363.83441162109375,
    "y2": 599.8590087890625
  }
}, {
  "renderDpi": 300,
  "name": "6",
  "page": 8,
  "figType": "Figure",
  "regionBoundary": {
    "x1": 57.839999999999996,
    "y1": 446.64,
    "x2": 539.04,
    "y2": 641.04
  },
  "caption": "Figure 6. Whisper is competitive with state-of-the-art commercial and open-source ASR systems in long-form transcription. The distribution of word error rates from six ASR systems on seven long-form datasets are compared, where the input lengths range from a few minutes to a few hours. The boxes show the quartiles of per-example WERs, and the per-dataset aggregate WERs are annotated on each box. Our model outperforms the best open source model (NVIDIA STT) on all datasets, and in most cases, commercial ASR systems as well.",
  "imageText": ["Whisper", "Company", "A", "Company", "B", "Company", "C", "Company", "D", "NVIDIA", "STT", "(CTC", "large)", "(%", ")", "at", "e", "ro", "r", "R", "d", "Er", "W", "or", "40", "35", "30", "25", "20", "15", "10", "5", "TED-LIUM3", "Meanwhile", "Kincaid46", "Rev16", "Earnings-21", "Earnings-22", "CORAAL0"],
  "renderURL": "/Users/shahrukh/Desktop/acad-gpt/Users/shahrukh/Desktop/acad-gpt/examples/paper_highlights/pdf/figures/de6b744efdf14a3c85ba5cb3f5fc3a77_whisper-Figure6-1.png",
  "captionBoundary": {
    "x1": 54.9379997253418,
    "y1": 658.8486938476562,
    "x2": 541.4442138671875,
    "y2": 708.0869750976562
  }
}, {
  "renderDpi": 300,
  "name": "1",
  "page": 4,
  "figType": "Table",
  "regionBoundary": {
    "x1": 70.8,
    "y1": 66.96,
    "x2": 271.2,
    "y2": 138.0
  },
  "caption": "Table 1. Architecture details of the Whisper model family.",
  "imageText": ["Tiny", "4", "384", "6", "39M", "Base", "6", "512", "8", "74M", "Small", "12", "768", "12", "244M", "Medium", "24", "1024", "16", "769M", "Large", "32", "1280", "20", "1550M", "Model", "Layers", "Width", "Heads", "Parameters"],
  "renderURL": "/Users/shahrukh/Desktop/acad-gpt/Users/shahrukh/Desktop/acad-gpt/examples/paper_highlights/pdf/figures/de6b744efdf14a3c85ba5cb3f5fc3a77_whisper-Table1-1.png",
  "captionBoundary": {
    "x1": 67.62699890136719,
    "y1": 149.60775756835938,
    "x2": 277.2563781738281,
    "y2": 155.010009765625
  }
}]
